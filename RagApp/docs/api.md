# Documentation for api 

## Overview
This FastAPI project provides endpoints for ingesting data into ChromaDB and querying it using our RAG architecture with LLM . The project includes two main endpoints: `/ingest` and `/query`.

## Models.py
This contains the model of the data being passed, we are sinply passing the question hance only the question : str attribute.
Any new data types being passed through the APIs should be defined and imported from here

## Routes.py

### 1. Ingest Endpoint

**URL:** `/ingest`  
**Method:** `POST`  
**Description:** The excel or csv file is loaded and the data is extracted. From this, wthe questions are extracted and the embed_texts() function is called to generate embeddings. These embeddings are then stored in chomaDB along with their unique ID using UUIDs.

**Request:**
- **file**: A CSV file containing data to be ingested.

**Response:**
- **Success:** A message indicating the number of records ingested.
- **Error:** An error message if the ingestion fails.
---
### 2. Query Endpoint

**URL:** `/query`  
**Method:** `POST`  
**Description:** This endpoint accepts the question to be queried. The query_chroma_with_similarity() function is invoked and the query is passed. The function returns the top 3 best results which correspond to top 3 queries in the DB which are similar to the user provided query. Along with this, it also provides the top1 best answer. The ask_llama() function is invoked to generate response from the LLM which goes through RAG architecture.

**Request:**
- **required:** An object of type QueryRequest defined in models.py

**Response:**

- **question:** The original question.
- **matches:** The top results from ChromaDB.
- **LLM answer:** The answer generated by the language model.